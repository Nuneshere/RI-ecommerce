{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tools\n",
    "from tools import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando a base de dados\n",
    "df = load_database()\n",
    "# criando o modelo\n",
    "vocabulary, model = bag_of_words(df['content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vocabulary.toarray()\n",
    "Y = df.label.values\n",
    "\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "#Treinar o classificador\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8309859154929577, 0.84375, 0.7941176470588235, 0.8181818181818182]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rodar no conjunto de teste\n",
    "predict_labels = classifier.predict(X_test)\n",
    "#Avaliar o classificador\n",
    "evaluate(Y_test, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"portuguese\"))\n",
    "USER_AGENT = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "              'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
    "\n",
    "def clean(text):\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    word = nfkd.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    d = re.sub(\"[^a-zA-Z ]\", \"\", word)\n",
    "    words = d.lower().split()\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    words = ' '.join(words)\n",
    "    return words\n",
    "\n",
    "def filter_tag(content):\n",
    "    if content.parent.name in [\"head\", \"title\", \"style\", \"script\", \"[document]\"]:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(content.encode('utf-8'))):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def content_page(url):\n",
    "    req = requests.get(url, headers=USER_AGENT)\n",
    "    content = req.text\n",
    "    s = BeautifulSoup(content, 'html.parser')\n",
    "    data = s.find_all(text=True)\n",
    "    data = ''.join(filter(filter_tag, data))\n",
    "    text = data.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_page(page):\n",
    "    #Pegar o content de page\n",
    "    text = content_page(page)\n",
    "    #Rodar o classificador treinado para esta p√°gina\n",
    "    features = model.transform([text])\n",
    "    return classifier.predict(features.toarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs = glob.glob(\"../Crawler/Bfs/*.csv\")\n",
    "heuristc = glob.glob(\"../Crawler/Heuristic/*.csv\")\n",
    "crawlers = [bfs, heuristc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, crawler in enumerate(crawlers):\n",
    "    t = \"bfs\" if i == 0 else \"heuristic\"\n",
    "    for csv in crawler:\n",
    "        results = []\n",
    "        df = pd.read_csv(csv).transpose()\n",
    "        df = df.reset_index()\n",
    "        df.columns = [\"Links\"]\n",
    "        store = csv.split(\"/\")[3]\n",
    "\n",
    "        for index, line in enumerate(df.values):\n",
    "            print(\"Classificando \" + line[0])\n",
    "            print((index / df.shape[0]) * 100, \"%\")\n",
    "            try:\n",
    "                results.append([line[0], classify_page(line[0])])\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print(\"Deu pau em: \", line)\n",
    "\n",
    "            dataframe = pd.DataFrame(results, columns=[\"url\", \"label\"])\n",
    "            dataframe.to_csv(path_or_buf=f\"../Results/re_{t}_{store}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristcs = glob.glob(\"../Results/Heuristic/*.csv\")\n",
    "frames = []\n",
    "for csv in heuristcs:\n",
    "    frames.append(pd.read_csv(csv))\n",
    "dataset = pd.concat(frames)\n",
    "dataset = dataset[dataset[\"label\"] == 1]\n",
    "dataset = dataset.reset_index().drop([\"index\"], axis=1)\n",
    "dataset.to_csv(path_or_buf=f\"../Results/positive_docs.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_re = '((www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z0-9]+(\\.[a-zA-Z0-9]+)*)'\n",
    "links = dataset[\"url\"].values\n",
    "for id, link in enumerate(links):\n",
    "    title = re.findall(url_re, link)[0][0] + \"_\" + str(id) + \".html\"\n",
    "    try:\n",
    "        req = requests.get(link, headers=USER_AGENT)\n",
    "        print(\"Baixando: \", title)\n",
    "        with open(title, \"w\") as f:\n",
    "            f.write(req.text)\n",
    "            time.sleep(1)\n",
    "    except:\n",
    "        !cd pages/ && curl -o {title} {link} && cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
