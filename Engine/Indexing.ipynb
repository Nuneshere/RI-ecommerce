{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import OrderedDict\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"portuguese\"))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"title\", \"price\", \"colors\", \"sizes\", \"description\", \"url\", \"html\"]\n",
    "df = pd.read_csv(\"../Results/wrapper_docs.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "df[\"price\"] = df[\"price\"].replace('[\\/R$,]', '', regex=True).apply(lambda x: float(x) / 100)\n",
    "prices = df[\"price\"]\n",
    "qrts = [prices.quantile(.25), prices.quantile(.5), prices.quantile(.75), prices.quantile(1)]\n",
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartile(value):\n",
    "    qrt = \"\"\n",
    "    if value >= qrts[3]:\n",
    "        qrt = \"4q\"\n",
    "    elif value >= qrts[2]:\n",
    "        qrt = \"3q\"\n",
    "    elif value >= qrts[1]:\n",
    "        qrt = \"2q\"\n",
    "    elif value > 0:\n",
    "        qrt = \"1q\"\n",
    "    else:\n",
    "        qrt = \"\"\n",
    "    return qrt\n",
    "df[\"price\"]  = df[\"price\"].apply(lambda x: quartile(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors(color):\n",
    "    color = \",\".join(color.split(\"/\"))\n",
    "    if \"white\" in color:\n",
    "        color = color.replace(\"white\", \"branco\")\n",
    "    elif \"blue\" in color:\n",
    "        color = color.replace(\"blue\", \"azul\")\n",
    "    elif \"tamanho único\" in color:\n",
    "        color = color.replace(\",tamanho único\", \"\")\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colors\"] = df[\"colors\"].apply(lambda x: x.lower() if not pd.isnull(x) else x)\n",
    "df[\"sizes\"] = df[\"sizes\"].apply(lambda x: x.lower() if not pd.isnull(x) else x)\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"colors\"] = df[\"colors\"].apply(lambda x: colors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lossy(words):\n",
    "    words = [w for w in words if w not in STOPWORDS]\n",
    "    return [stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    nfkd = unicodedata.normalize('NFKD', str(word))\n",
    "    doc = nfkd.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    d = re.sub(\"[^a-zA-Z0-9]\", \" \", doc)\n",
    "    words = d.lower().split()\n",
    "    return filter_lossy(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os termos dos documents, ordena os termos e retorna a ED: termo.campo -> (Id do document, frequencia)\n",
    "def extract_terms(documents):\n",
    "    terms = []\n",
    "    for idx, doc in enumerate(documents):\n",
    "        for field, info in enumerate(doc[:-2]):\n",
    "            if not pd.isnull(info):\n",
    "                tokens = list(map(lambda x: x.lower(), info)) if isinstance(info, list) else tokenize(info)\n",
    "                for token in tokens:\n",
    "                    field_index = token + '.' + columns[field]\n",
    "                    pair = (field_index, (idx, tokens.count(token)))\n",
    "                    if pair not in terms:\n",
    "                        terms.append(pair)\n",
    "    return sorted(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprime os postings\n",
    "def compress_postings(postings):\n",
    "    compressed = [postings[0]]\n",
    "    last = postings[0][0]\n",
    "    for posting in postings[1:]:\n",
    "        doc_id, freq = posting\n",
    "        compressed.append((doc_id - last, freq))\n",
    "        last = doc_id\n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_postings(postings):\n",
    "    decompress = [postings[0]]\n",
    "    last = postings[0][0]\n",
    "    for posting in postings[1:]:\n",
    "        doc_id, freq = posting\n",
    "        decompress.append((doc_id + last, freq))\n",
    "        last += doc_id\n",
    "    return decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Íncide como uma ED termo -> postings\n",
    "def create_index(documents, use_compression=True):\n",
    "    terms = extract_terms(documents)\n",
    "    index = OrderedDict()\n",
    "    for term, doc_id in terms:\n",
    "        index.setdefault(term, []).append(doc_id)\n",
    "    if use_compression:\n",
    "        for term, postings in index.items():\n",
    "            index[term] = compress_postings(postings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no-compress.json', 'w') as f:\n",
    "    f.write(json.dumps(create_index(df.values, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708\n"
     ]
    }
   ],
   "source": [
    "obj = {'content' : 'something goes here'}\n",
    "json_obj = json.load(open('./compress.json'), object_pairs_hook=OrderedDict)\n",
    "json_size = len(json_obj)\n",
    "print(json_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "statinfo = os.stat(\"../Results/wrapper_docs.csv\")\n",
    "size = int(statinfo.st_size / 1024)\n",
    "rows.append([\"List Documents\", size])\n",
    "\n",
    "statinfo = os.stat(\"./compress.json\")\n",
    "size = int(statinfo.st_size / 1024)\n",
    "rows.append([\"Index Compressed\", size])\n",
    "\n",
    "statinfo = os.stat(\"./no-compress.json\")\n",
    "size = int(statinfo.st_size / 1024)\n",
    "rows.append([\"Index Uncompressed\", size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows, columns=[\"Data\", \"Size (KB)\"]).to_csv(path_or_buf=r\"index_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
